{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqFU102UlgyG"
      },
      "source": [
        "## Naive Bayes Classifier: A Probabilistic Approach to Classification\n",
        "\n",
        "The Naive Bayes classifier is a simple yet powerful algorithm for **text classification tasks** like sentiment analysis. It works by making the **assumption of conditional independence** between features (words in a text). This assumption allows it to efficiently calculate the probability of a text belonging to a specific class (e.g., positive or negative sentiment) based on the individual probabilities of its constituent words.\n",
        "\n",
        "**Here's a brief overview of the steps involved:**\n",
        "\n",
        "**1. Training:**\n",
        "\n",
        "* Analyze a training dataset containing labeled texts (e.g., positive and negative tweets).\n",
        "* Calculate the **probability of each word** appearing in each class (positive and negative).\n",
        "* Calculate the **prior probability** of each class (e.g., the proportion of positive and negative examples in the training data).\n",
        "\n",
        "**2. Prediction:**\n",
        "\n",
        "* For a new, unseen text:\n",
        "    * Estimate the **probability of each word appearing** in the text given each class (positive or negative) using the probabilities calculated during training.\n",
        "    * Combine these individual word probabilities using **Bayes' theorem** to get the **overall probability** of the text belonging to each class.\n",
        "    * **Classify the text** to the class with the **highest probability**.\n",
        "\n",
        "**Benefits:**\n",
        "\n",
        "* **Simple to implement and understand:** Easy to explain and interpret compared to some complex models.\n",
        "* **Efficient training:** Requires less training data compared to some other algorithms.\n",
        "* **Effective for text classification:** Performs well on tasks like sentiment analysis and spam filtering.\n",
        "\n",
        "**Limitations:**\n",
        "\n",
        "* **Conditional independence assumption:** May not hold true in real-world data, potentially affecting accuracy.\n",
        "* **Sensitivity to rare words:** Can struggle with words that rarely appear in the training data.\n",
        "\n",
        "**Overall, the Naive Bayes classifier offers a robust and efficient approach to text classification tasks**, making it a popular choice for applications like sentiment analysis and spam filtering. It's a good starting point for beginners due to its simplicity and interpretability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "iaz04ESytaae"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8yxU9XH0tmj8"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import TweetTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Qw-FnpcnDZ9"
      },
      "source": [
        "## Filtering the Tweet Text to remove unwanted noise in the tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hcRS6_RUt7jd"
      },
      "outputs": [],
      "source": [
        "def process_tweet(tweet):\n",
        "    '''\n",
        "    Input:\n",
        "        tweet: a string containing a tweet\n",
        "    Output:\n",
        "        tweets_clean: a list of words containing the processed tweet\n",
        "\n",
        "    '''\n",
        "\n",
        "    # Create a custom stopwords list excluding \"not\"\n",
        "    stopwords_english = list(stopwords.words('english'))\n",
        "    # Remove \"not\" from the list\n",
        "    # remove stock market tickers like $GE\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    # remove hyperlinks\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    # tokenize tweets\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        if (word not in stopwords_english and  # remove stopwords\n",
        "            word not in string.punctuation):  # remove punctuation\n",
        "            # Lemmatization instead of stemming\n",
        "            lemmatizer = WordNetLemmatizer()\n",
        "            lemma_word = lemmatizer.lemmatize(word)\n",
        "            tweets_clean.append(lemma_word)\n",
        "\n",
        "    return tweets_clean\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PGVXAfSDuKUa"
      },
      "outputs": [],
      "source": [
        "def test_lookup(func):\n",
        "    freqs = {('sad', 0): 4,\n",
        "             ('happy', 1): 12,\n",
        "             ('oppressed', 0): 7}\n",
        "    word = 'happy'\n",
        "    label = 1\n",
        "    if func(freqs, word, label) == 12:\n",
        "        return 'SUCCESS!!'\n",
        "\n",
        "    return 'Failed Sanity Check!'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "D7VKFOet1UTe"
      },
      "outputs": [],
      "source": [
        "def lookup(freqs, word, label):\n",
        "    '''\n",
        "    Input:\n",
        "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
        "        word: the word to look up\n",
        "        label: the label corresponding to the word\n",
        "    Output:\n",
        "        n: the number of times the word with its corresponding label appears.\n",
        "    '''\n",
        "    n = 0  # freqs.get((word, label), 0)\n",
        "\n",
        "    pair = (word, label)\n",
        "    if (pair in freqs):\n",
        "        n = freqs[pair]\n",
        "\n",
        "    return n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhqpHl_Z14vh",
        "outputId": "f9a45a3e-7cb3-4454-8f35-8e5c3eaf11fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SUCCESS!!\n"
          ]
        }
      ],
      "source": [
        "# Define test data\n",
        "freqs = {('sad', 0): 4,\n",
        "         ('happy', 1): 12,\n",
        "         ('oppressed', 0): 7}\n",
        "\n",
        "# Call test function\n",
        "result = test_lookup(lookup)\n",
        "\n",
        "# Print the result\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "{\n",
        "    ('sad', 0): 4,\n",
        "    ('happy', 1): 12,\n",
        "    ('oppressed', 0): 7\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drYOm27XnQY9"
      },
      "source": [
        "### Importing Useful Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "h8eoOjBc2Wfj"
      },
      "outputs": [],
      "source": [
        "import pdb\n",
        "from nltk.corpus import stopwords, twitter_samples\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from os import getcwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DK3Mtu_5337I",
        "outputId": "eb418b3d-3a64-41be-e030-1c36aa3141c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('twitter_samples')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0tI4k0P3-ak",
        "outputId": "1e9654d7-3a2e-4ece-ac13-9864ee5cafb7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drINfaEWnblI"
      },
      "source": [
        "### Using Sample Tweets from the Natural Language ToolKit(NLTK)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mIyh4AmG4bph"
      },
      "outputs": [],
      "source": [
        "# get the sets of positive and negative tweets\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "622nV1OW4kl_",
        "outputId": "e38ef5c9-48cc-4581-8790-fc6ba1ac413c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)',\n",
              " '@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!',\n",
              " '@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!',\n",
              " '@97sides CONGRATS :)',\n",
              " 'yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_positive_tweets[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIwyOT8U4pWF",
        "outputId": "70d9f3ee-338d-4857-d481-3a3fd7dd7a64"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['hopeless for tmr :(',\n",
              " \"Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\",\n",
              " '@Hegelbon That heart sliding into the waste basket. :(',\n",
              " '“@ketchBurning: I hate Japanese call him \"bani\" :( :(”\\n\\nMe too',\n",
              " 'Dang starting next week I have \"work\" :(']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_negative_tweets[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msWhLyuJnnsF"
      },
      "source": [
        "### Splitting the Training and Test Data Set in 7:3 ratio\n",
        "* training_data = 0.7 * total_data\n",
        "* test_data = 0.3 * total_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RmxULcFL409L"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split positive tweets\n",
        "train_pos, test_pos = train_test_split(all_positive_tweets, test_size=0.3, random_state=42)\n",
        "\n",
        "# Split negative tweets\n",
        "train_neg, test_neg = train_test_split(all_negative_tweets, test_size=0.3, random_state=42)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qPgjHlzG5XUY"
      },
      "outputs": [],
      "source": [
        "train_x = train_pos + train_neg\n",
        "test_x = test_pos + test_neg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GkTjngrf5mp0"
      },
      "outputs": [],
      "source": [
        "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
        "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZ-ApBjS6XpN",
        "outputId": "4be60897-0d16-4395-97c4-a6213ac19962"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1., 1., 1., ..., 0., 0., 0.])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIVvDQAX6gFC",
        "outputId": "ba62759b-c7ca-4854-c136-fc526bcce3ab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1., 1., 1., ..., 0., 0., 0.])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Flj08KkZ7u8-",
        "outputId": "74eef39f-1ef0-4afd-e94a-83a81724410e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPhDXCrw6iTc",
        "outputId": "14fc2dc4-71b3-461e-abd7-1eac7884a0d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['hello', 'great', 'day', ':)', 'good', 'morning']\n"
          ]
        }
      ],
      "source": [
        "custom_tweet = \"RT @Twitter @chapagain not nor Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
        "\n",
        "# print cleaned tweet\n",
        "print(process_tweet(custom_tweet))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "SfL9TVqD64xU"
      },
      "outputs": [],
      "source": [
        "def count_tweets(result, tweets, ys):\n",
        "    '''\n",
        "    Input:\n",
        "        result: a dictionary that will be used to map each pair to its frequency\n",
        "        tweets: a list of tweets\n",
        "        ys: a list corresponding to the sentiment of each tweet (either 0 or 1)\n",
        "    Output:\n",
        "        result: a dictionary mapping each pair to its frequency\n",
        "    '''\n",
        "    for y, tweet in zip(ys, tweets):\n",
        "        for word in process_tweet(tweet):\n",
        "            # define the key, which is the word and label tuple\n",
        "            pair = (word,y)\n",
        "\n",
        "            # if the key exists in the dictionary, increment the count\n",
        "            if pair in result:\n",
        "                result[pair] += 1\n",
        "\n",
        "            # else, if the key is new, add it to the dictionary and set the count to 1\n",
        "            else:\n",
        "                result[pair] = 1\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "d3CtrS3-auwe"
      },
      "outputs": [],
      "source": [
        "freqs = count_tweets({}, train_x, train_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "PAQxpX-mYMM3"
      },
      "outputs": [],
      "source": [
        "import itertools\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zF-_WdC_YOkE",
        "outputId": "2d21ba0f-bab7-444d-c726-4563256f622e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{('aaahhh', 1.0): 1,\n",
              " ('see', 1.0): 118,\n",
              " ('...', 1.0): 215,\n",
              " (':)', 1.0): 2481,\n",
              " ('blogged', 1.0): 1}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dict(itertools.islice(freqs.items(), 5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lO3Ii04oLuD",
        "outputId": "8771c999-ac8f-414b-a35d-1841f8e23d02"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "11441"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(freqs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEC1CVygfFW5",
        "outputId": "8a70ea4e-9b42-40d9-b7c5-4fca12f94a24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OrderedDict([(('landlord', 0.0), 1), ((\"mp's\", 0.0), 1), (('apt', 0.0), 1), (('building', 0.0), 1), (('bldg', 0.0), 1), (('shouldve', 0.0), 1), (('muster', 0.0), 1), (('merchs', 0.0), 1), (('cancelling', 0.0), 1), (('needicecreamnow', 0.0), 1), (('livestream', 0.0), 1), (('vitamin', 0.0), 1), (('oil', 0.0), 1), (('healthier', 0.0), 1), (('stretch', 0.0), 1), (('himseek', 0.0), 1), (('kikmsn', 0.0), 1), (('kissme', 0.0), 1), (('akua', 0.0), 1), (('owns', 0.0), 1)])\n"
          ]
        }
      ],
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "last_20_items = OrderedDict(list(freqs.items())[-20:])\n",
        "print(last_20_items)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRon8U1joXZi"
      },
      "source": [
        "# Defining the Naive Bayes Function to train the training data set\n",
        "\n",
        "Naive bayes is an algorithm that could be used for sentiment analysis. It takes a short time to train and also has a short prediction time.\n",
        "\n",
        "##### So how do we train a Naive Bayes classifier?\n",
        "</br>\n",
        "Given a freqs dictionary, `train_x` (a list of tweets) and a `train_y` (a list of labels for each tweet), implement a naive bayes classifier.\n",
        "\n",
        "##### Calculate $V$\n",
        "- You can then compute the number of unique words that appear in the `freqs` dictionary to get your $V$ (you can use the `set` function).\n",
        "\n",
        "##### Calculate $freq_{pos}$ and $freq_{neg}$\n",
        "- Using your `freqs` dictionary, you can compute the positive and negative frequency of each word $freq_{pos}$ and $freq_{neg}$.\n",
        "\n",
        "##### Calculate $N_{pos}$ and $N_{neg}$\n",
        "- Using `freqs` dictionary, you can also compute the total number of positive words and total number of negative words $N_{pos}$ and $N_{neg}$.\n",
        "\n",
        "##### Calculate $D$, $D_{pos}$, $D_{neg}$\n",
        "- Using the `train_y` input list of labels, calculate the number of documents (tweets) $D$, as well as the number of positive documents (tweets) $D_{pos}$ and number of negative documents (tweets) $D_{neg}$.\n",
        "- Calculate the probability that a document (tweet) is positive $P(D_{pos})$, and the probability that a document (tweet) is negative $P(D_{neg})$\n",
        "\n",
        "##### Calculate the logprior\n",
        "- the logprior is $log(D_{pos}) - log(D_{neg})$\n",
        "\n",
        "##### Calculate log likelihood\n",
        "- Finally, you can iterate over each word in the vocabulary, use your `lookup` function to get the positive frequencies, $freq_{pos}$, and the negative frequencies, $freq_{neg}$, for that specific word.\n",
        "- Compute the positive probability of each word $P(W_{pos})$, negative probability of each word $P(W_{neg})$ using equations 4 & 5.\n",
        "\n",
        "$$ P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V}\\tag{4} $$\n",
        "$$ P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V}\\tag{5} $$\n",
        "\n",
        "**Note:** We'll use a dictionary to store the log likelihoods for each word.  The key is the word, the value is the log likelihood of that word).\n",
        "\n",
        "- You can then compute the loglikelihood: $log \\left( \\frac{P(W_{pos})}{P(W_{neg})} \\right)\\tag{6}$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "5pEzYq_sY6mt"
      },
      "outputs": [],
      "source": [
        "def train_naive_bayes(freqs, train_x, train_y):\n",
        "    '''\n",
        "    Input:\n",
        "        freqs: dictionary from (word, label) to how often the word appears\n",
        "        train_x: a list of tweets\n",
        "        train_y: a list of labels correponding to the tweets (0,1)\n",
        "    Output:\n",
        "        logprior: the log prior. (equation 3 above)\n",
        "        loglikelihood: the log likelihood of you Naive bayes equation. (equation 6 above)\n",
        "    '''\n",
        "    loglikelihood = {}\n",
        "    logprior = 0\n",
        "\n",
        "\n",
        "    # calculate V, the number of unique words in the vocabulary\n",
        "    vocab = set([pair[0] for pair in freqs.keys()])\n",
        "    V = len(vocab)\n",
        "\n",
        "    # calculate N_pos, N_neg, V_pos, V_neg\n",
        "    N_pos=N_neg=V_pos=V_neg=0\n",
        "    for pair in freqs.keys():\n",
        "        # if the label is positive (greater than zero)\n",
        "        if pair[1] > 0:\n",
        "            # increment the count of unique positive words by 1\n",
        "            V_pos += 1\n",
        "\n",
        "            # Increment the number of positive words by the count for this (word, label) pair\n",
        "            N_pos += freqs[pair]\n",
        "\n",
        "        # else, the label is negative\n",
        "        else:\n",
        "            # increment the count of unique negative words by 1\n",
        "            V_neg += 1\n",
        "\n",
        "            # increment the number of negative words by the count for this (word,label) pair\n",
        "            N_neg += freqs[pair]\n",
        "\n",
        "    # Calculate D, the number of documents\n",
        "    D = train_y.shape[0]\n",
        "\n",
        "    # Calculate D_pos, the number of positive documents\n",
        "    D_pos = train_y[train_y == 1].shape[0]\n",
        "\n",
        "    # Calculate D_neg, the number of negative documents\n",
        "    D_neg = train_y[train_y == 0].shape[0]\n",
        "\n",
        "    # Calculate logprior\n",
        "    logprior  = np.log(D_pos / D) - np.log(D_neg / D)\n",
        "\n",
        "    # For each word in the vocabulary...\n",
        "    for word in vocab:\n",
        "        # get the positive and negative frequency of the word\n",
        "        freq_pos = freqs.get((word, 1), 0)\n",
        "        freq_neg = freqs.get((word, 0), 0)\n",
        "\n",
        "        # calculate the probability that each word is positive, and negative\n",
        "        p_w_pos = (freq_pos + 1) / (N_pos + V)\n",
        "        p_w_neg = (freq_neg + 1) / (N_neg + V)\n",
        "\n",
        "        # calculate the log likelihood of the word\n",
        "        loglikelihood[word] = np.log(p_w_pos / p_w_neg)\n",
        "\n",
        "\n",
        "    return logprior, loglikelihood"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLTCoaqmdznP",
        "outputId": "1408c008-88b6-4501-dd43-fe7e6a587d1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0\n",
            "9239\n"
          ]
        }
      ],
      "source": [
        "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n",
        "print(logprior)\n",
        "print(len(loglikelihood))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GzLRT2nqYLo"
      },
      "source": [
        "## Naive Bayes Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "I2D-yfwTfViJ"
      },
      "outputs": [],
      "source": [
        "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
        "    '''\n",
        "    Input:\n",
        "        tweet: a string\n",
        "        logprior: a number\n",
        "        loglikelihood: a dictionary of words mapping to numbers\n",
        "    Output:\n",
        "        p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n",
        "\n",
        "    '''\n",
        "    # process the tweet to get a list of words\n",
        "    word_l = process_tweet(tweet)\n",
        "\n",
        "    # initialize probability to zero\n",
        "    p = 0\n",
        "\n",
        "    # add the logprior\n",
        "    p += logprior\n",
        "\n",
        "    for word in word_l:\n",
        "\n",
        "        # check if the word exists in the loglikelihood dictionary\n",
        "        if word in loglikelihood:\n",
        "            # add the log likelihood of that word to the probability\n",
        "            p += loglikelihood[word]\n",
        "\n",
        "    return p\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PscY3GhPi9B3"
      },
      "source": [
        "# Negative Sentiment Prompted Tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxhWl1C_h2Xl",
        "outputId": "db5dbc1a-41be-4caf-b72d-673ee5b1fd03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The expected output is -2.194295493603877\n"
          ]
        }
      ],
      "source": [
        "my_tweet = 'Bad weather.'\n",
        "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
        "print('The expected output is', p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwIQearMjN3e"
      },
      "source": [
        "# Positive Sentiment Prompted Tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUX4olVEi2x2",
        "outputId": "d84757da-c941-4436-fc23-97f11aee9563"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The expected output is 1.6558807823524089\n"
          ]
        }
      ],
      "source": [
        "my_tweet = 'Sunny day.'\n",
        "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
        "print('The expected output is', p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4ocJ_NKi7Gx",
        "outputId": "f48b0da7-f0ad-4704-826b-5144ba2b06e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Post a Tweet!Karma is a bitch.\n",
            "Negative Tweet\n"
          ]
        }
      ],
      "source": [
        "new_tweet = input(\"Post a Tweet!\")\n",
        "p = naive_bayes_predict(new_tweet, logprior, loglikelihood)\n",
        "if p>0 :\n",
        "  print(\"Positive Tweet\")\n",
        "else:\n",
        "  print(\"Negative Tweet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BJbw40wj5Ze",
        "outputId": "9460b245-bcaf-4301-9247-6eaea5dc2220"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Post a Tweet!Trump start crime in usa\n",
            "Negative Tweet\n"
          ]
        }
      ],
      "source": [
        "new_tweet = input(\"Post a Tweet!\")\n",
        "p = naive_bayes_predict(new_tweet, logprior, loglikelihood)\n",
        "if p>0 :\n",
        "  print(\"Positive Tweet\")\n",
        "else:\n",
        "  print(\"Negative Tweet\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHe58JVkqd6n"
      },
      "source": [
        "## Accuracy Measurement of Our Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2mhxxJckKI-"
      },
      "outputs": [],
      "source": [
        "def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        test_x: A list of tweets\n",
        "        test_y: the corresponding labels for the list of tweets\n",
        "        logprior: the logprior\n",
        "        loglikelihood: a dictionary with the loglikelihoods for each word\n",
        "    Output:\n",
        "        accuracy: (# of tweets classified correctly)/(total # of tweets)\n",
        "    \"\"\"\n",
        "    accuracy = 0  # return this properly\n",
        "\n",
        "    y_hats = []\n",
        "    for tweet in test_x:\n",
        "        # if the prediction is > 0\n",
        "        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
        "            # the predicted class is 1\n",
        "            y_hat_i = 1\n",
        "        else:\n",
        "            # otherwise the predicted class is 0\n",
        "            y_hat_i = 0\n",
        "\n",
        "        # append the predicted class to the list y_hats\n",
        "        y_hats.append(y_hat_i)\n",
        "\n",
        "    # error is the average of the absolute values of the differences between y_hats and test_y\n",
        "    error = sum(abs(test_y-y_hats))/len(y_hats)\n",
        "\n",
        "    # Accuracy is 1 minus the error\n",
        "    accuracy = 1-error\n",
        "\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lv3pUZSkoyM",
        "outputId": "fec24ff0-4092-4811-a627-132d410f3243"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive Bayes accuracy = 0.9900\n"
          ]
        }
      ],
      "source": [
        "print(\"Naive Bayes accuracy = %0.4f\" %\n",
        "      (test_naive_bayes(test_x, test_y, logprior, loglikelihood)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
